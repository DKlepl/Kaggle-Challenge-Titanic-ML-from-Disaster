---
title: "Data Cleaning and Exploration"
author: "Dominik Klepl"
date: "12 2 2018"
output: html_document
---
# 1. Data Analysis

###Load required libraries and data
```{r setup, include=FALSE}
library(lmerTest)
library(caret)
library(pROC)
library(dplyr)
library(ggplot2)
library(nnet)

train = readr::read_csv("Data/train.csv")
test = readr::read_csv("Data/test.csv")
```

## 1.1 Prepare data
I expect there will be some missing values in both training and test data and other unclean aspects of the data. Therefore in following code chunk I merge training and test data together into a big dataframe. In order to split them later again into test and training data I add a binary feature 'train' where datapoints with 1 are the training data and 0 are the test data.
```{r}
#add survived column to the test data
test$Survived = NA

data_all = rbind(test,train)

data_all$train = 1
data_all$train[is.na(data_all$Survived)]= 0

#check that train=1 are really only the train data
sum(data_all$train==1)==nrow(train)
```


## 1.2 First data check
First I need to check whether all features are the correct class I need them to be.
```{r data classes}
str(data_all)
```

There are missing values in Age, Cabin and Embarked. 
#todo: deal with NAs

Next the balance of both classes (survived/died) needs to be checked to avoid excesive bias of the model.
```{r balance of classes}
sum(train$Survived==0) #549 survived
sum(train$Survived==1) #343 died

#compute no-information rate = if the model only guessed and marked all as Survived what accuracy it would have
(sum(train$Survived==0)/length(train$Survived))*100

#Survived needs to be factor
train$Survived = as.factor(train$Survived)
data_all$Survived = as.factor(data_all$Survived)
```

The imbalance is not so pronounced but should be dealt with if the model behaves unpredictably.
#todo: balance classes

## 1.2 Data exploration
Now explore the features in the dataset and look at their distributions. In case of continuous feature make a histogram. With categorical ones a bar plot is more effective.

Start with Pclass. Most people are in 3rd class and most of these also died. Most survivors are in 1st class. In 2nd class it's 50-50.
When the data is split also by gender we can see that almost all women in 1st class survived. For men being in 1st was best but it did not matter much whether they were in 2nd or 3rd class.

#todo: feature engineer combination of class and sex
```{r Pclass}
#Pclass feature coloured by number of survivors
ggplot(train,aes(x=Pclass,fill=Survived))+
  geom_bar(stat='count', position='stack')
  
#check Pclass split by gender
ggplot(train,aes(x=Pclass,fill=Survived))+
  geom_bar(stat='count', position='stack')+
  facet_grid(~Sex)
```

Now sex. Majority of passengers were males (64.7 %). However only 18.9 % of them survived. We can assume that men left a place in boats to women. Which supports the fact that 74% of them survived. This big difference is likely a very good predictor.
```{r Sex}
#also sex should be a factor for plotting purposes
train$Sex = as.factor(train$Sex)

(sum(train$Sex=="male")/891)*100 #64.76 males

ggplot(train,aes(x=Sex,fill=Survived))+
  geom_bar(stat='count',position='stack')

#how many women survived then?
sum(train$Sex=="female" & train$Survived==1)/sum(train$Sex=="female")*100 #74% survived

#and men?
sum(train$Sex=="male" & train$Survived==1)/sum(train$Sex=="male")*100 #18.9% survived
```

Now onto age. There are some likely hypotheses to draw from the distribution of age split by survivors. Children were the most likely to survive which makes sense. We can also assume that if parents let their children on boat then they probably died. That supports the fact that most dead were between 20 and 35. These could have been either the parents or singles or mix of both probably.

There are also a lot of missing values. These need to be filled in by using feature engineering.
#todo: predict age to fill NAs
```{r Age}
train$Age = as.factor(train$Age)

ggplot(train,aes(x=Age,fill=Survived))+
  geom_bar(stat="density")
```

The fare is probably not very good predictor but there are nonetheless some insights. As we already saw from the class feature most dead were in the 3rd class. These people were actually likely not to be passengers but ship crew and therefore their fare was 0. From the histogram of fare we can see that indeed those that paid nothing were more likely to die. Whereas those with the most expensive (but also exclusive) tickets were very likely to survive.

It might be useful to engineer a new feature to separate crew from the passengers.
#todo: create feature: crew (either 1 or 0)
```{r Fare}
ggplot(train,aes(x=Fare,fill=Survived))+
  geom_bar(stat="density")
```

With the place where the passengers embarked it seems there is a mild trend of people embarking in S(Southampton) dying more. However it's somewhat difficult to confirm this from the plot, therefore I test this hypothesis with a simple linear model.
```{r Embarked}
ggplot(train,aes(x=Embarked,fill=Survived))+
  geom_bar(stat='count',position='stack')

#test the likelihood of dying based on place of embarkment
embark_model = glm(Survived ~ Embarked, data=train,family="binomial")
summary(embark_model)
```

As it turns out the Embarked feature is indeed a significant predictor of survival with Southampton having the most dead. It is likely this feature will be used in the model and therefore we need to have this information available for all people. We'll train a model that will predict Embarked based on all the other info later after we engineer all new features.

##TODO predict Embarked

```{r function for evaluating performance of models}
performance = function (data) {
  data$Predictions[data$PredictionsPerc>0.5]='1'
  data$Predictions[data$PredictionsPerc<=0.5]='0'
  c_matrix=confusionMatrix(data = data$Predictions, reference = data$Survived,positive='0')
  
  print(c_matrix)
  #extract the measurements
  Accuracy=c_matrix$overall[[1]]
  Sensitivity=c_matrix$byClass[[1]]
  Specificity=c_matrix$byClass[[2]]
  PPV=c_matrix$byClass[[3]]
  NPV=c_matrix$byClass[[4]]
  
  #ROC
  rocCurve <- roc(response = data$Survived, predictor =data$PredictionsPerc)
  a=auc(rocCurve)#from 0 to 1 (+ being good)
  AUC=a[1]
  ROC_plot=plot(rocCurve, legacy.axes = TRUE)
  
  
  output = cbind(Accuracy,Sensitivity,Specificity,PPV,NPV,AUC)
  output=as.data.frame(output)
  print(c_matrix)
  print(ROC_plot)
  return(output)
}
```

```{r cross-validation function}
crossvalidate = function(model,nfold,data) {
  predictions = data.frame()
  `%not in%` <- function (x, table) is.na(match(x, table, nomatch=NA_integer_))
  Folds = createFolds(unique(data$PassengerId),nfold)
  for (f in Folds) {
    train_d = subset(data, PassengerId %not in% f)
    #subset including only 1 fold
    test_d = subset(data, PassengerId %in% f)
    #fit train subset into specified model
    model_val = update(model, data = train_d)
    test_d$PredictionsPerc=predict(model_val,test_d,type="response",allow.new.levels=TRUE)
    prediction_fold = data.frame(PassengerId=test_d$PassengerId,Survived=test_d$Survived,PredictionsPerc=test_d$PredictionsPerc)
    predictions = rbind(predictions,prediction_fold)
  }

  result = performance(predictions)
  
  return(result)
}
```

## 1.3 Feature Engineering

Let's follow up on some of the ##TODOs for unfolding some of the insights gained in previous section. Because we need these new features also in the test data all of this will be done in data_all. To plot the relationships of the new features train dataset will be used.

First create a column: Class_sex i.e 1_M, 1_F and plot to see the effect of this new feature on rate of survivors. We saw
```{r class+sex}
data_all=tidyr::unite(data_all, Class_sex, c(Pclass, Sex), remove=FALSE)

data_all$Class_sex=as.factor(data_all$Class_sex)

ggplot(subset(data_all,train==1),aes(x=Class_sex,fill=Survived))+
  geom_bar(stat="count", position="stack")
```

The column name might have also a lot of information. Therefore split the strings in "name" column into several features: Title and Surname.
```{r}
#extract surnames - split a string into part before comma and after it and saves first part
data_all$Surname = sapply(data_all$Name, function(x) {strsplit(x, split='[,]')[[1]][1]})
#some of the surnames had also a maiden name included - get rid of that
data_all$Surname = sapply(data_all$Surname, function(x) {strsplit(x, split='[-]')[[1]][1]})

#similarly with extracting title - only difference, split string by "," and "."
data_all$Title = sapply(data_all$Name, function(x) {strsplit(x, split='[,.]')[[1]][2]})
#now the title has a space before first letter - that's not desired
data_all$Title = sub(" ","",data_all$Title)

#explore and summarize titles into less categories
#what kind of titles are there?
Titles = as.factor(data_all$Title)
levels(Titles)

#women
data_all$Title[data_all$Title=="Ms"] = "Miss"
data_all$Title[data_all$Title=="Mlle"] = "Miss"
data_all$Title[data_all$Title=="Mme"] = "Mrs"
data_all$Title[data_all$Title=="Lady"] = "Mrs"
data_all$Title[data_all$Title=="the Countess"] = "Mrs"
data_all$Title[data_all$Title=="Dona"] = "Mrs"

#men
data_all$Title[data_all$Title=="Don"] = "Mr"
data_all$Title[data_all$Title=="Sir"] = "Mr"
data_all$Title[data_all$Title=="Dr"] = "Mr"

#rare titles
data_all$Title[data_all$Title=="Capt"] = "Rare"
data_all$Title[data_all$Title=="Col"] = "Rare"
data_all$Title[data_all$Title=="Jonkheer"] = "Rare"
data_all$Title[data_all$Title=="Rev"] = "Rare"
data_all$Title[data_all$Title=="Major"] = "Rare"
```

Now that the titles are extracted and grouped into smaller number of categories one can see that Masters (although not big in size) were very likely to survive. Similar counts for Misses and Mrs'. Misters on the other hand as expected from the "Sex" feature were less likely to survive. Then there is the rest of "rare" titles. As there are titles such as Captain and Major it is likely that these people as being part of the crew were doomed from the beginning.
```{r}
#turn Title into factor and check the result
data_all$Title = as.factor(data_all$Title)
levels(data_all$Title)

#how did the passengers survived based on their titles
ggplot(subset(data_all,train==1), aes(x=Title,fill=Survived))+
  geom_bar(stat="count", position = "stack")
```


Another new feature can be engineered from linear combination of SibSp(number of siblings or spouse) and Parch (number of guardians of children). It's size of a family. This is simply SibSP+Parch+1 (for the passenger herself). Then plot this new feature to see how it affected survival.
```{r}
data_all$Family_size=data_all$SibSp+data_all$Parch+1
data_all$Family_size=as.factor(data_all$Family_size)

ggplot(subset(data_all,train==1), aes(x=Family_size,fill=Survived))+
  geom_bar(stat="count", position="stack")
```

Tickets-single people but travelling with non-family (friends)
```{r}
TicketGroup <- data_all %>%
        select(Ticket) %>%
        group_by(Ticket) %>%
        summarise(Ticket_people=n())
data_all <- left_join(data_all, TicketGroup, by = "Ticket")

#how many of such non-family groups were there?
sum(data_all$Ticket_people[data_all$Family_size==1]>1) #127 not much but still might get better info than family size of 1

#since family size and ticket_people might overlap often let's keep only one of them, the one with bigger number

data_all$Group = NA

#now family size is a factor, I need to turn it back to numeric
data_all$Family_size = as.numeric(data_all$Family_size)

for (i in 1:nrow(data_all)) {
  data_all$Group[i] = max(data_all$Family_size[i], data_all$Ticket_people[i])
}

data_all$Group=as.factor(data_all$Group)

ggplot(subset(data_all,train==1),aes(x=Group,fill=Survived))+
  geom_bar(stat="count", position="stack")
```

```{r crew members}
data_all$Crew=0
data_all$Crew[data_all$Fare==0]=1

sum(data_all$Crew==1)

range(data_all$Age[data_all$Crew==0],na.rm=T)
range(data_all$Age[data_all$Crew==1],na.rm=T)

ggplot(subset(data_all,train==1),aes(x=Crew,fill=Survived))+
  geom_bar(stat="count", position="stack")

#how many of them are in test data
sum(data_all$Crew[data_all$train==0]==1) #only two but any information might help in the end
```



```{r predict Embarked}
sum(is.na(data_all$Embarked)) #2 missing values

#maybe they were part of a group or family?
data_all$Group[is.na(data_all$Embarked)] #they were both from a group of 2 people
data_all$Family_size[is.na(data_all$Embarked)] #but they weren't there with family
#could they be traveling together?
data_all$Ticket[is.na(data_all$Embarked)] #seems to be true

#let's figure out where they came from

#are they in test or train data?
data_all$train[is.na(data_all$Embarked)] #both in train data

#train a model that can predict embark
#create a subset with only usable features
embark_data = data_all

predict_embark = multinom(Embarked~Class_sex+Pclass+Sex+Age+SibSp+Parch+Fare+Title+Group,data=embark_data)
embark_data$Predictions=predict(predict_embark,embark_data, type="class",allow.new.levels=T)
confusionMatrix(data = embark_data$Predictions, reference = embark_data$Embarked)

#the model doesn't do a very good job but it predicts the NAs to be from C

#let's check why the model thinks they are from C
#maybe some of the places had more wealthy passengers than any other, let's look at differences in Fare
emb_fare = lm(Fare ~ Embarked,embark_data)
summary(emb_fare) #they indeed do differ

#since the NAs survived and payed 80 for the ticket they are unlikely to come from Q
#it's S or C
emb_fare2 = lm(Fare ~ Embarked, subset(embark_data,Embarked!="Q"))
summary(emb_fare2)

#the average fare in S was 27.46 => it's more probable that the NAs came from C - merge the models prediction for the 2 NAs with the original data
embark_NA = subset(embark_data,is.na(Embarked))
embark_NA$Embarked=predict(predict_embark,embark_NA, type="class",allow.new.levels=T)


data_all$Embarked[data_all$PassengerId %in% embark_NA$PassengerId] = embark_NA$Embarked

sum(is.na(data_all$Embarked))

write.csv(data_all,"Data/cleaner_data_all.csv",row.names = F)
```

Now we fill in the missing values in age. We train a model that predicts age, this time we're aiming for a great performance as age seems to have a large effect on survival and also there is a lot of missing values in age column.

```{r predict age}
age_d = data_all
sum(is.na(age_d$Age)) # 264 NAs

age_d_c=subset(age_d,!is.na(Age))

#turn all required features to factor
age_d$Class_sex=as.integer(age_d$Class_sex)
age_d$Group=as.integer(age_d$Group)
```

To evaluate the performance of my models I'll need a function for cross-validation. There are some functions in package however in this case I prefer to have full control over the process. Therefore let's build such function now.
```{r}
#building function for cross validation
crossvalidation  = function (model,d) {
  `%not in%` <- function (x, table) is.na(match(x, table, nomatch=NA_integer_))
  Folds = createFolds(unique(d$PassengerId),10)
  result={}
  for (i in Folds) {
    #subset of Fixations_visa except for the 1 fold
    train_d = subset(d, PassengerId %not in% i)
    train_d=subset(train_d,!is.na(train_d$Age))
    #subset including only 1 fold
    test_d = subset(d,  PassengerId %in% i)
    test_d=subset(test_d,!is.na(test_d$Age))
    #fit train subset into specified model
    model_val = update(model,data=train_d)
    #rmse of train data
    pred1 = predict(model_val, train_d,allow.new.levels=T)
    rmse_train=Metrics::rmse(train_d$Age[!is.na(pred1)],pred1[!is.na(pred1)])
    
    #rmse of test data
    pred2 = predict(model_val, test_d,allow.new.levels=T)
    rmse_test=Metrics::rmse(test_d$Age[!is.na(pred2)],pred2[!is.na(pred2)])
    
    res = cbind(rmse_train,rmse_test)
    result=rbind(result,res)
    
  }
  
  final_res=data.frame(mean_train=mean(result[1]),mean_test=mean(result[2]))
  return(final_res)
}
```

Now that everything is prepared we can start fitting models.
```{r create models}
#linear regression is always a good start
age_1 = lmer(Age ~Pclass+Sex+Fare+Embarked+Title+Group+(1|Surname),data=age_d_c)
age_2 = lm(Age ~Pclass+Sex+Fare+Embarked+Title+Group,data=age_d_c)
age_3 = lmer(Age ~Pclass+Fare+Embarked+Title+Group+(1|Surname),data=age_d_c)
age_4 = lmer(Age ~Pclass+Sex+Fare+Embarked+Title+Group+Crew+(1|Surname),data=age_d_c)

crossvalidation(age_1,age_d_c)
crossvalidation(age_2,age_d_c)
crossvalidation(age_3,age_d_c)
crossvalidation(age_4,age_d_c)
```


